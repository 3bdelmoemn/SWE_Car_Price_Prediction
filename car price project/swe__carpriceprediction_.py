# -*- coding: utf-8 -*-
"""SWE _CarPricePrediction .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LIj08cIMlFDYFzs7N9kPZ2ahA2-5S1YS

# Libraries:
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder,PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb

"""# Load Data:"""

df=pd.read_csv("/content/car_price_prediction.csv")

print(f'Dataset includes {df.shape[0]} rows and {df.shape[1]} columns')

df.columns=df.columns.str.lower().str.replace(' ','_')
print(f'Features :\n{df.drop("price", axis=1).columns.to_list()}')
print('-'*50)
print(f'Target :\n{["price"]}')

df.info()

object_cols = list(df.select_dtypes(include='object').columns)
numeric_cols = list(df.select_dtypes(include=['int64', 'float64']).columns)
print(f'NO.of Object columns : {len(object_cols)}')
print(f'Object columns : {object_cols}')
print('-'*50)
print(f'NO.of Numeric columns : {len(numeric_cols)}')
print(f'Numeric columns : {numeric_cols}')

df.head()

df.tail()

"""# Data Cleaning:"""

# check duplicate:
print(f'Duplicate rows : {df.duplicated().sum()}')

df.drop_duplicates(inplace=True)

print(f'Duplicate rows : {df.duplicated().sum()}')

# check null values :
print(f'Null Values :\n{df.isnull().sum()}','-'*50,f'Total: {df.isnull().sum().sum()} ',sep='\n')

df.dropna( inplace=True)

print(f'Null Values :\n{df.isnull().sum()}','-'*50,f'Total: {df.isnull().sum().sum()} ',sep='\n')

# text treatment:
for col in object_cols:
  df[col]=df[col].str.lower()
  if col=='levy':
    df[col]=df[col].str.replace('-','0')
    df[col]=df[col].str.replace('.','')
  if col=='engine_volume':
    df[col]=df[col].str.strip('turbo')
  if col=='mileage':
    df[col]=df[col].str.strip('_km')
  if col=='doors':
    df[col]=df[col].str.replace('-may','')
    df[col]=df[col].str.replace('-mar','')
    df[col]=df[col].str.replace('0','')
    df[col]=df[col].str.replace('>','')

# print some of unique value for each object col:
for col in object_cols:
  print(f'{col} : {df[col].unique()[:10]}')
  print('-'*50)

df.head()

for col in object_cols:
  if df[col].str.isnumeric().all():
    df[col]=df[col].astype('int')

object_cols = list(df.select_dtypes(include='object').columns)
numeric_cols = list(df.select_dtypes(include=['int64', 'float64']).columns)
print(f'NO.of Object columns : {len(object_cols)}')
print(f'Object columns : {object_cols}')
print('-'*50)
print(f'NO.of Numeric columns : {len(numeric_cols)}')
print(f'Numeric columns : {numeric_cols}')

df.head()

# 1- treat levy col:
df.levy.value_counts()

df.levy.describe()

med = df['levy'].replace(0, np.nan).median()  # Compute the median ignoring zeros
df['levy'] = df['levy'].replace(0, med)       # Replace zeros with the median
print('Done')

# 2- treat fuel_type col:
df.fuel_type.value_counts()

df = df[df['fuel_type'].isin(['petrol', 'diesel', 'hybrid'])]
print('Done')

# 3- treat mileage col:
df.mileage=df.mileage.astype('int')

# 4- treat manufacturer col:
df.manufacturer.value_counts().nlargest(6)

df=df[df.manufacturer.isin(['toyota','hyundai','mercedes-benz','ford','chevrolet','bmw'])]
df.rename(columns={'manufacturer':'brand'},inplace=True)

# 5- treat model col:
df.drop('model',axis=1,inplace=True)

# 6- treat category col:
df.category.value_counts().nlargest(6)

df=df[df.category.isin(['sedan','jeep','hatchback','minivan','coupe','microbus'])]

# 7- treat engine_volume col:
df.engine_volume=df.engine_volume.astype('float64')

# 8- treat gear_box_type col:
df.gear_box_type.value_counts()

df.gear_box_type.replace('tiptronic','automatic',inplace=True)
df=df[df.gear_box_type.isin(['automatic','manual'])]
df.rename(columns={'gear_box_type':'type'},inplace=True)

# 9 wheel:
df.wheel.value_counts()

df.wheel.replace('left wheel','left',inplace=True)
df.wheel.replace('right-hand drive','right',inplace=True)

# 10- color column:
df.color.value_counts().nlargest(7)

df=df[df.color.isin(['black','white','red','blue','silver','grey','green'])]

# 11- treat prod_year:
df['car_age']=2024-df['prod._year']

# 12- drop id ,prod_year:
df.drop(['id','prod._year'],axis=1,inplace=True)

df.doors.value_counts()

df=df[df.doors.isin([2,4])]
df.doors=df.doors.astype('object')

df.cylinders.value_counts()

# update price:
df.price=df.price*100

object_cols = list(df.select_dtypes(include='object').columns)
numeric_cols = list(df.select_dtypes(include=['int64', 'float64']).columns)
print(f'NO.of Object columns : {len(object_cols)}')
print(f'Object columns : {object_cols}')
print('-'*50)
print(f'NO.of Numeric columns : {len(numeric_cols)}')
print(f'Numeric columns : {numeric_cols}')

df.head()

print(f'Dataset includes {df.shape[0]} rows and {df.shape[1]} columns')

# save cleaned data:
# df.to_csv('car_price_Dataset_cleaned.csv',index=False)

"""# Exploratory Data Analysis:"""

# load cleaned Data:
# df=pd.read_csv('/content/car_price_Dataset_cleaned.csv',dtype={'doors': 'object'})

print(f'Dataset includes {df.shape[0]} rows and {df.shape[1]} columns')

df.head()

df.tail()

df.info()

object_cols=list(df.select_dtypes(include=['object']))
numeric_cols=list(df.select_dtypes(include=['int64','float64']))
print(f'NO.of Object columns : {len(object_cols)}')
print(f'Object columns : {object_cols}')
print('-'*50)
print(f'NO.of Numeric columns : {len(numeric_cols)}')
print(f'Numeric columns : {numeric_cols}')

print('-'*50,'Statistical values of Numerical Data:','-'*50,sep='\n')
df[numeric_cols].describe()

print('-'*50,'Details of Categorical Data:','-'*50,sep='\n')
df[object_cols].describe()

print('-'*50,'Skewness of Numerical Data: ','-'*50,sep='\n')
df[numeric_cols].skew()

# plot Distribution of all Numerical Data:
plt.figure(figsize=(15,10))
for i,col in enumerate(numeric_cols):
    plt.subplot(4,2,i+1)
    sns.histplot(df[col], bins=50, kde=True)
plt.suptitle('Distribution of all Numerical Data:')
plt.tight_layout()
plt.show()

# print value counts for each object attributes:
for col in object_cols:
  print(f'{df[col].value_counts()}')
  print('-'*50)

# plot value count for each attribute:
plt.figure(figsize=(15,10))
for i,col in enumerate(object_cols):
  plt.subplot(5,2,i+1)
  sns.countplot(data=df[object_cols],x=col,hue=col)
plt.suptitle('Value Count for each attribute:',fontsize=14)
plt.tight_layout()
plt.show()

# print correlation between all numerical attributes:
corr=df[numeric_cols].corr()
print(f'correlation:\n---------------\nprice vs. numerical attributes:\n---------------\n{corr.price.sort_values(ascending=False)}')

# plot correlation:
plt.figure(figsize=(10,10))
sns.heatmap(corr,annot=True,cmap='Blues')
plt.title('Correlation between all numerical attributes:')
plt.show()

# plot relation between price and another numerical values:
plt.figure(figsize=(15,10))
for i,col in enumerate(numeric_cols):
  plt.subplot(4,2,i+1)
  sns.regplot(data=df[numeric_cols],x=col,y='price',line_kws={'color':'orange'},scatter_kws={'color':'green', 'alpha':0.5})
plt.suptitle('Relation between price and another numerical values:',fontsize=14)
plt.tight_layout()
plt.show()

# plot relation between price and categorical attributes:

for col in object_cols:
  plt.figure(figsize=(10,10))
  sns.boxplot(data=df,x=col,y='price',hue=col)
  plt.show()

"""# Data Preprocessing:"""

df[numeric_cols].describe()

# show skewness of numerical features:
df[numeric_cols].skew()

# Plot box plot to show outliers in numerical features:
plt.figure(figsize=(15,10))
for i,col in enumerate(numeric_cols):
  plt.subplot(4,2,i+1)
  sns.boxplot(data=df[numeric_cols],x=col)
plt.suptitle('Box plot to show outliers in numerical features:',fontsize=14)
plt.tight_layout()
plt.show()

# modified outlier:
q3=df[numeric_cols].quantile(0.75)
q1=df[numeric_cols].quantile(0.25)
iqr=q3-q1
upper_fence=q3+1.5*iqr
lower_fence=q1-1.5*iqr
for col in numeric_cols:
  df[col]=df[col].mask(df[col]>upper_fence[col],upper_fence[col])
  df[col]=df[col].mask(df[col]<lower_fence[col],lower_fence[col])

df[numeric_cols].describe()

df[numeric_cols].skew()

# Plot box plot to show outliers in numerical features:
plt.figure(figsize=(15,10))
for i,col in enumerate(numeric_cols):
  plt.subplot(4,2,i+1)
  sns.boxplot(data=df[numeric_cols],x=col)
plt.suptitle('Box plot to show outliers in numerical features:',fontsize=14)
plt.tight_layout()
plt.show()

numeric_cols = ['levy', 'engine_volume', 'mileage', 'cylinders', 'airbags', 'car_age']

# create processor:
cat_encoder=OneHotEncoder(drop='first',handle_unknown='ignore')
num_scaler=MinMaxScaler()
data_transformer=ColumnTransformer([('cat',cat_encoder,object_cols),('num',num_scaler,numeric_cols)],remainder='passthrough')
data_transformer

# transform price:
df['price']=MinMaxScaler().fit_transform(df[['price']])

df.price

# transorm data and Encoding:
df_without_price=df.drop('price',axis=1)
transformed_data=data_transformer.fit_transform(df_without_price)
cat_feature_names=data_transformer.named_transformers_['cat'].get_feature_names_out(object_cols)
all_feature_names=list(cat_feature_names)+numeric_cols
df_temp=pd.DataFrame(transformed_data,columns=all_feature_names,index=df.index)
df_temp['price']=df.price
df=df_temp

# save transformer to use again :
# joblib.dump(data_transformer,'data_transformer_preprocessing.pkl')

df.corr().price.sort_values(ascending=False)

# separate features and target:
X=df.drop('price',axis=1)
y=df.price

X.head()

y.head()

# split training ,testing:
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

# print size of train and test
print(f'x_train shape : {x_train.shape}')
print(f'x_test shape : {x_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

"""# Models:"""

# create Evaluation Function for print Result:
def evaluate_model(model,x_train,y_train,x_test,y_test):
  model.fit(x_train,y_train)

  y_pred_train=model.predict(x_train)
  mse_train=mean_squared_error(y_train,y_pred_train)
  rmse_train=np.sqrt(mse_train)
  mae_train=mean_absolute_error(y_train,y_pred_train)
  r2_train=r2_score(y_train,y_pred_train)

  y_pred_test=model.predict(x_test)
  mse_test=mean_squared_error(y_test,y_pred_test)
  rmse_test=np.sqrt(mse_train)
  mae_test=mean_absolute_error(y_test,y_pred_test)
  r2_test=r2_score(y_test,y_pred_test)

  print('Model summary:','-'*50,sep='\n')
  print('Training:\n',f'MSE={mse_train}\n RMSE={rmse_train}\n MAE={mae_train}\n R2={r2_train}')
  print('-'*50)
  print('Testing:\n',f'MSE={mse_test}\n RMSE={rmse_test}\n MAE={mae_test}\n R2={r2_test}')
  return mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test

details=dict()

# 1- LinearRegression:
lr=LinearRegression()
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(lr,x_train,y_train,x_test,y_test)
details['LinearRegression']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 2- apply PCA:
pca=PCA(n_components=15)
x_train_pca=pca.fit_transform(x_train)
x_test_pca=pca.transform(x_test)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(lr,x_train_pca,y_train,x_test_pca,y_test)
details['LinearRegression_pca']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 3- apply ploynomial feature :
poly=PolynomialFeatures(degree=2)
x_train_poly=poly.fit_transform(x_train_pca)
x_test_poly=poly.transform(x_test_pca)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(lr,x_train_poly,y_train,x_test_poly,y_test)
details['LinearRegression_poly']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 4- Random Forrest:
rf_model = RandomForestRegressor(
    n_estimators=500,
    max_depth=10,
    min_samples_split=9,
    min_samples_leaf=8,
    random_state=42
)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(rf_model,x_train,y_train,x_test,y_test)
details['RandomForrest']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 5- Random Forrest with pca:
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=4,
    max_features='sqrt',
    max_samples=0.8,
    random_state=42
)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(rf_model,x_train_pca,y_train,x_test_pca,y_test)
details['RandomForrest_pca']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 6- gradientboost:
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(gb_model,x_train,y_train,x_test,y_test)
details['GradientBoost']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

# 7- XGB:
xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.06,max_depth=8,min_child_weight=4,reg_alpha=2,reg_lambda=12)
mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(xgb_model,x_train,y_train,x_test,y_test)
details['XGBoosting']=[mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test]

Details=pd.DataFrame(details,index=['mse_train','rmse_train','mae_train','r2_train','mse_test','rmse_test','mae_test','r2_test'])
Details

"""# Best Model : "XGBoosting"
"""

xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.06,max_depth=8,min_child_weight=4,reg_alpha=2,reg_lambda=12)

xgb_model.fit(x_train,y_train)

y_pred=xgb_model.predict(x_test)

y_pred

mse_train,rmse_train,mae_train,r2_train,mse_test,rmse_test,mae_test,r2_test=evaluate_model(xgb_model,x_train,y_train,x_test,y_test)

# plot result:
sns.scatterplot(x=np.arange(20),y=y_pred[:20],color='red',label='Predict')
sns.scatterplot(x=np.arange(20),y=y_test[:20],color='green',label='True')
plt.legend()
plt.title('Predict vs. True')
plt.show()

# Save model:
# joblib.dump(xgb_model,'xgb_model.pkl')

"""# app:"""

import joblib
import numpy as np
import pandas as pd
def test_model(data):
    model=joblib.load(r'/content/xgb_model.pkl')
    transformer=joblib.load(r'/content/data_transformer_preprocessing.pkl')
    columns=['levy', 'brand', 'category', 'leather_interior', 'fuel_type',
       'engine_volume', 'mileage', 'cylinders', 'type', 'drive_wheels',
       'doors', 'wheel', 'color', 'airbags', 'car_age']
    df_data=pd.DataFrame([data],columns=columns)
    transformed_data=transformer.transform(df_data)
    y_pred=model.predict(transformed_data)
    final_result=(y_pred*(5.443288e+06-1.000000e+02))+1.000000e+02
    return final_result[0]

test_data=[831,'hyundai','sedan',	'yes',	'petrol',2.4,161600,	4.0	, 'automatic','front','4','left','black',8,13]
print(test_model(test_data))
# 1568100



